{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8nX-9-3uVtS",
    "outputId": "1c152d74-4e17-4f3b-9178-5c0c232268a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  2 10:49:04 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   70C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces8F2ZQLVyu"
   },
   "source": [
    "### **Install/Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYSLL_p7uchD"
   },
   "outputs": [],
   "source": [
    "%pip install -q \"monai[nibabel, tqdm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xse2YBhAKf1Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import U2NET\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchsummary import summary\n",
    "\n",
    "import monai\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    AsDiscrete,\n",
    "    CastToTyped,\n",
    "    LoadNiftid,\n",
    "    Orientationd,\n",
    "    RandAffined,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandFlipd,\n",
    "    RandGaussianNoised,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.transforms import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFzKnVozK1Pd"
   },
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "batch_size = 1\n",
    "\n",
    "keys = ('image', 'label')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-35lAA-u3vC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-03Rxpg_u9o7"
   },
   "source": [
    "### **Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8FycWMEvAFx"
   },
   "outputs": [],
   "source": [
    "DIR = '/content/drive/MyDrive/COVID-19-20/Train'\n",
    "\n",
    "images = sorted(glob.glob(os.path.join(DIR, '*_ct.nii.gz')))\n",
    "labels = sorted(glob.glob(os.path.join(DIR, '*_seg.nii.gz')))\n",
    "n_train = int(train_split * len(images)) + 1\n",
    "n_val = int(len(images) - n_train)\n",
    "\n",
    "train_data = [{keys[0]: img, keys[1]: seg} for img,seg in zip(images[:n_train], labels[:n_train])]\n",
    "val_data = [{keys[0]: img, keys[1]: seg} for img,seg in zip(images[-n_val:], labels[-n_val:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcldqZOXvBUp"
   },
   "outputs": [],
   "source": [
    "def get_transforms(mode='train', keys=('image', 'label')):\n",
    "\n",
    "    if mode == 'train':\n",
    "        transform = Compose([\n",
    "                             LoadNiftid(keys),\n",
    "                             AddChanneld(keys),\n",
    "                             Orientationd(keys, axcodes = \"LPS\"),\n",
    "                             Spacingd(keys, pixdim=(1.25, 1.25, 3.0), mode=(\"bilinear\", \"nearest\")[: len(keys)]),\n",
    "                             ScaleIntensityRanged(keys[0], a_min=-1000.0, a_max=500.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "                             SpatialPadd(keys, spatial_size=(192, 192, -1), mode=\"reflect\"),          # ensure at least 192x192\n",
    "                             RandAffined(keys,\n",
    "                                         prob=0.15,\n",
    "                                         rotate_range=(-0.05, 0.05), scale_range=(-0.1, 0.1),\n",
    "                                         mode=(\"bilinear\", \"nearest\"),\n",
    "                                         as_tensor_output=False),\n",
    "                             RandCropByPosNegLabeld(keys, label_key=keys[1], spatial_size=(192, 192, 16), num_samples=4),\n",
    "                             RandGaussianNoised(keys[0], prob=0.15, std=0.01),\n",
    "                             RandFlipd(keys, spatial_axis=0, prob=0.5),\n",
    "                             RandFlipd(keys, spatial_axis=1, prob=0.5),\n",
    "                             RandFlipd(keys, spatial_axis=2, prob=0.5),\n",
    "                             CastToTyped(keys, dtype=(np.float32, np.uint8)),\n",
    "                             ToTensord(keys)\n",
    "                            ])\n",
    "    if mode == 'val':\n",
    "        transform = Compose([\n",
    "                             LoadNiftid(keys),\n",
    "                             AddChanneld(keys),\n",
    "                             Orientationd(keys, axcodes = \"LPS\"),\n",
    "                             Spacingd(keys, pixdim=(1.25, 1.25, 3.0), mode=(\"bilinear\", \"nearest\")[: len(keys)]),\n",
    "                             ScaleIntensityRanged(keys[0], a_min=-1000.0, a_max=500.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "                             SpatialPadd(keys, spatial_size=(192, 192, -1), mode=\"reflect\"),          # ensure at least 192x192 if not then apply padd\n",
    "                             RandCropByPosNegLabeld(keys, label_key=keys[1], spatial_size=(192, 192, 16), num_samples=4),\n",
    "                             CastToTyped(keys, dtype=(np.float32, np.uint8)),\n",
    "                             ToTensord(keys)\n",
    "                            ])\n",
    "    if mode == 'infer':\n",
    "        transform = Compose([\n",
    "                             LoadNiftid(keys),\n",
    "                             AddChanneld(keys),\n",
    "                             Orientationd(keys, axcodes = \"LPS\"),\n",
    "                             Spacingd(keys, pixdim=(1.25, 1.25, 3.0), mode=(\"bilinear\", \"nearest\")[: len(keys)]),\n",
    "                             ScaleIntensityRanged(keys[0], a_min=-1000.0, a_max=500.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "                             CastToTyped(keys, dtype=(np.float32,)),\n",
    "                             ToTensord(keys)\n",
    "                            ])\n",
    "    return transform\n",
    "\n",
    "train_transforms = get_transforms('train', keys)\n",
    "val_transforms = get_transforms('val', keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gFR27Wxxqw9"
   },
   "outputs": [],
   "source": [
    "# Create DataSet\n",
    "\n",
    "train_ds = Dataset(data = train_data,\n",
    "                   transform = train_transforms)\n",
    "val_ds = Dataset(data = val_data,\n",
    "                 transform = val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAXybRaHxqt7"
   },
   "outputs": [],
   "source": [
    "# Set DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size=batch_size)\n",
    "val_loader = DataLoader(val_ds,\n",
    "                        batch_size=batch_size)         # image-level batch to the sliding window method, not the window-level batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnjV5WVixt0H"
   },
   "outputs": [],
   "source": [
    "# explore DataLoader\n",
    "\n",
    "print('Training data Info:')\n",
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "images,labels = data['image'],data['label']\n",
    "print(\"shape of images : {}\".format(images.shape))\n",
    "print(\"shape of labels : {}\".format(labels.shape))\n",
    "\n",
    "print('\\nValidation data Info:')\n",
    "dataiter = iter(val_loader)\n",
    "data = dataiter.next()\n",
    "images,labels = data['image'],data['label']\n",
    "print(\"shape of images : {}\".format(images.shape))\n",
    "print(\"shape of labels : {}\".format(labels.shape))\n",
    "\n",
    "del dataiter, data, images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThKQslprxvfE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AN-EEvJmvDs5"
   },
   "source": [
    "### **Create Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvMtBmiIvGga",
    "outputId": "092edcb1-8311-4e1b-aaca-23a7b9ae36ff"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv3d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.BatchNorm3d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "model = U2NET(in_ch=1, out_ch=1).to(device)\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK20Hu58vGeW"
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 192, 192, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EvaAt_kfMI-"
   },
   "source": [
    "### **Optimizer, Scheduler and Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfkD25YqfONd"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=9, min_lr=0.00001, verbose=True)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, axis=(2, 3, 4), epsilon=0.00001):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def get_loss(self, y_pred, y_true):\n",
    "        dice_numerator = (2.0 * torch.sum(y_pred * y_true, axis=self.axis)) + self.epsilon\n",
    "        dice_denominator = torch.sum(y_pred**2, dim=self.axis) + torch.sum(y_true**2, dim=self.axis) + self.epsilon\n",
    "        dice_loss = 1 - torch.mean(dice_numerator / dice_denominator)\n",
    "        return dice_loss\n",
    "\n",
    "    def forward(self, d0, d1, d2, d3, d4, d5, d6, labels):\n",
    "        loss0 = self.get_loss(d0, labels)\n",
    "        loss1 = self.get_loss(d1, labels)\n",
    "        loss2 = self.get_loss(d2, labels)\n",
    "        loss3 = self.get_loss(d3, labels)\n",
    "        loss4 = self.get_loss(d4, labels)\n",
    "        loss5 = self.get_loss(d5, labels)\n",
    "        loss6 = self.get_loss(d6, labels)\n",
    "\n",
    "        loss = (loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6) / 7\n",
    "        return loss\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, axis=(2, 3, 4), epsilon=0.00001):\n",
    "    dice_numerator = (2.0 * torch.sum(y_pred * y_true, axis=axis)) + epsilon\n",
    "    dice_denominator = torch.sum(y_pred, dim=axis) + torch.sum(y_true, dim=axis) + epsilon\n",
    "    dice_coefficient = torch.mean(dice_numerator / dice_denominator)\n",
    "    return dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x97ZBoyfvcR1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck6_FoRovG-q"
   },
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWvjKVDDvejf"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "hist_train_loss = []\n",
    "hist_val_loss = []\n",
    "hist_train_dice = []\n",
    "hist_val_dice = []\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "soft_dice_loss = DiceLoss()\n",
    "PATH = '/content/drive/MyDrive/COVID-19-20/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48PKu6GPvha8"
   },
   "source": [
    "#### *From Begining*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Q3R5CgqUvmMm",
    "outputId": "47e60d91-77ba-449e-a67b-612c9761cb0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [39:12<00:00, 14.70s/it]\n",
      "100%|██████████| 39/39 [05:17<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1: \n",
      "Train loss:      0.8336648002266884, \tTrain Dice:      0.09252109378576279, \n",
      "Validation loss: 0.7061381317101992, \tValidation Dice: 0.19312192499637604\n",
      "Learning Rate: {0.001}\n",
      "Validation loss is decreased from inf ---> 0.7061381317101992.\n",
      "Saving Model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:44<00:00, 10.03s/it]\n",
      "100%|██████████| 39/39 [02:30<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2: \n",
      "Train loss:      0.737138481810689, \tTrain Dice:      0.17908094823360443, \n",
      "Validation loss: 0.7256916593282651, \tValidation Dice: 0.2031329870223999\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:43<00:00, 10.02s/it]\n",
      "100%|██████████| 39/39 [02:27<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3: \n",
      "Train loss:      0.7116575364023447, \tTrain Dice:      0.20767350494861603, \n",
      "Validation loss: 0.8067365510341449, \tValidation Dice: 0.13234709203243256\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:30<00:00,  9.94s/it]\n",
      "100%|██████████| 39/39 [02:30<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4: \n",
      "Train loss:      0.7063496949151158, \tTrain Dice:      0.22521273791790009, \n",
      "Validation loss: 0.7182975373206995, \tValidation Dice: 0.22539862990379333\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:51<00:00, 10.07s/it]\n",
      "100%|██████████| 39/39 [02:32<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5: \n",
      "Train loss:      0.7104199239052832, \tTrain Dice:      0.22383485734462738, \n",
      "Validation loss: 0.7222568400395222, \tValidation Dice: 0.21088379621505737\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:57<00:00, 10.11s/it]\n",
      "100%|██████████| 39/39 [02:31<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6: \n",
      "Train loss:      0.6646099496632815, \tTrain Dice:      0.263853520154953, \n",
      "Validation loss: 0.7170091561782055, \tValidation Dice: 0.21115851402282715\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [26:52<00:00, 10.08s/it]\n",
      "100%|██████████| 39/39 [02:27<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7: \n",
      "Train loss:      0.6810004183091223, \tTrain Dice:      0.25827500224113464, \n",
      "Validation loss: 0.7519041276895083, \tValidation Dice: 0.1833687424659729\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 138/160 [23:03<03:40, 10.03s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bd2c2d6fa72e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_dice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB (GPU 0; 14.73 GiB total capacity; 12.55 GiB already allocated; 425.88 MiB free; 13.38 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    num_train_sample = 0.0\n",
    "    num_val_sample = 0.0\n",
    "\n",
    "    dice_sum = 0.\n",
    "    dice_count = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        #optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        d, d1, d2, d3, d4, d5, d6 = model(inputs)\n",
    "        loss = soft_dice_loss(d, d1, d2, d3, d4, d5, d6, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "        num_train_sample += inputs.size(0)\n",
    "        dice = dice_coefficient(labels, d)\n",
    "        dice_sum += dice\n",
    "        dice_count += 1\n",
    "        del d, d1, d2, d3, d4, d5, d6, loss              # del temporary outputs and loss\n",
    "    train_dice = dice_sum / dice_count\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    hist_train_loss.append(train_loss)\n",
    "    hist_train_dice.append(train_dice)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dice_sum = 0.\n",
    "        dice_count = 0\n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            d, d1, d2, d3, d4, d5, d6 = model(inputs)\n",
    "            loss = soft_dice_loss(d, d1, d2, d3, d4, d5, d6, labels)\n",
    "            val_loss += loss.item()*inputs.size(0)\n",
    "            num_val_sample += inputs.size(0)\n",
    "            dice = dice_coefficient(labels, d)\n",
    "            dice_sum += dice\n",
    "            dice_count += 1\n",
    "        val_dice = dice_sum / dice_count\n",
    "        val_loss = val_loss / num_val_sample\n",
    "        hist_val_loss.append(val_loss)\n",
    "        hist_val_dice.append(val_dice)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch+1}: \\nTrain loss:      {train_loss}, \\tTrain Dice:      {train_dice}, \\nValidation loss: {val_loss}, \\tValidation Dice: {val_dice}')\n",
    "    print('Learning Rate:', {optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    if val_loss <= val_loss_min:\n",
    "        print(f'Validation loss is decreased from {val_loss_min} ---> {val_loss}.\\nSaving Model ...')\n",
    "        torch.save(model.state_dict(), PATH+'U2net3D_AutoEncoder_model.pth')\n",
    "        val_loss_min = val_loss\n",
    "\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'hist_train_loss': hist_train_loss,\n",
    "                'hist_val_loss': hist_val_loss,\n",
    "                'hist_train_dice': hist_train_dice,\n",
    "                'hist_val_dice': hist_val_dice,\n",
    "                'val_loss_min': val_loss_min},\n",
    "               PATH + 'U2net3D_AutoEncoder_model_checkpoints.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exg4lGETvm1l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xo-F3mDjvnKb"
   },
   "source": [
    "#### *Training from checkpoints*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs_oUurQvrEg"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(PATH + 'U2net3D_AutoEncoder_model_checkpoints.pt')\n",
    "\n",
    "epoch = checkpoint['epoch'] + 1\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "optimizer.defaults['lr'] = 0.0001\n",
    "\n",
    "hist_train_loss = checkpoint['hist_train_loss']\n",
    "hist_val_loss = checkpoint['hist_val_loss']\n",
    "hist_train_dice = checkpoint['hist_train_dice']\n",
    "hist_val_dice = checkpoint['hist_val_dice']\n",
    "val_loss_min = checkpoint['val_loss_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rllo7A_fvrxI",
    "outputId": "2e2b669b-64dc-4036-841a-0d489d925178"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [25:50<00:00,  9.69s/it]\n",
      "100%|██████████| 39/39 [02:27<00:00,  3.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10: \n",
      "Train loss:      0.6596331609413028, \tTrain Dice:      0.28460660576820374, \n",
      "Validation loss: 0.8296552575551547, \tValidation Dice: 0.13870349526405334\n",
      "Learning Rate: {0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 108/160 [17:58<09:19, 10.76s/it]"
     ]
    }
   ],
   "source": [
    "while epoch < epochs:\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    num_train_sample = 0.0\n",
    "    num_val_sample = 0.0\n",
    "\n",
    "    dice_sum = 0.\n",
    "    dice_count = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        #optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        inputs = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        d, d1, d2, d3, d4, d5, d6 = model(inputs)\n",
    "        loss = soft_dice_loss(d, d1, d2, d3, d4, d5, d6, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "        num_train_sample += inputs.size(0)\n",
    "        dice = dice_coefficient(labels, d)\n",
    "        dice_sum += dice\n",
    "        dice_count += 1\n",
    "        del d, d1, d2, d3, d4, d5, d6, loss              # del temporary outputs and loss\n",
    "    train_dice = dice_sum / dice_count\n",
    "    train_loss = train_loss / num_train_sample\n",
    "    hist_train_loss.append(train_loss)\n",
    "    hist_train_dice.append(train_dice)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dice_sum = 0.\n",
    "        dice_count = 0\n",
    "        for batch in tqdm(val_loader):\n",
    "            inputs = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            d, d1, d2, d3, d4, d5, d6 = model(inputs)\n",
    "            loss = soft_dice_loss(d, d1, d2, d3, d4, d5, d6, labels)\n",
    "            val_loss += loss.item()*inputs.size(0)\n",
    "            num_val_sample += inputs.size(0)\n",
    "            dice = dice_coefficient(labels, d)\n",
    "            dice_sum += dice\n",
    "            dice_count += 1\n",
    "        val_dice = dice_sum / dice_count\n",
    "        val_loss = val_loss / num_val_sample\n",
    "        hist_val_loss.append(val_loss)\n",
    "        hist_val_dice.append(val_dice)\n",
    "\n",
    "    print(f'\\nEpoch: {epoch+1}: \\nTrain loss:      {train_loss}, \\tTrain Dice:      {train_dice}, \\nValidation loss: {val_loss}, \\tValidation Dice: {val_dice}')\n",
    "    print('Learning Rate:', {optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    if val_loss <= val_loss_min:\n",
    "        print(f'Validation loss is decreased from {val_loss_min} ---> {val_loss}.\\nSaving Model ...')\n",
    "        torch.save(model.state_dict(), PATH+'U2net3D_AutoEncoder_model.pth')\n",
    "        val_loss_min = val_loss\n",
    "\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'hist_train_loss': hist_train_loss,\n",
    "                'hist_val_loss': hist_val_loss,\n",
    "                'hist_train_dice': hist_train_dice,\n",
    "                'hist_val_dice': hist_val_dice,\n",
    "                'val_loss_min': val_loss_min},\n",
    "               PATH + 'U2net3D_AutoEncoder_model_checkpoints.pt')\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l96NLmSPvrpk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ces8F2ZQLVyu",
    "cAAtzFOoLObE",
    "827tDEGiLSW2",
    "jCVsYHYJXO-d"
   ],
   "name": " U2-Net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
